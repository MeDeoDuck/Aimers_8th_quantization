"""
íŒŒì´í”„ë¼ì¸:
1. Scale Pruning (50% ì±„ë„ â†’ 0)
2. Distillation (Pruning ì†ì‹¤ ë³µêµ¬)
3. GPTQ 4-bit (ìµœì¢… ì••ì¶•)
"""

import os
import torch
import torch.nn as nn
import shutil
import torch.nn.functional as F
from datasets import load_dataset
from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments

from llmcompressor import oneshot
from llmcompressor.modifiers.quantization import GPTQModifier


MODEL_ID = "../Model/EXAONE-4.0-1.2B"
OUT_DIR = "./model_exp25"
DATASET_ID = "LGAI-EXAONE/MANTA-1M"

NUM_DISTILL_SAMPLES = 2048
NUM_CALIBRATION_SAMPLES = 2048
MAX_SEQUENCE_LENGTH = 512

PRUNING_RATIO = 0.5
LEARNING_RATE = 2e-5
NUM_EPOCHS = 1
TEMPERATURE = 2.0

print("=" * 70)
print("Scale Pruning + Distillation + GPTQ")
print("ìµœì  íŒŒì´í”„ë¼ì¸!")
print("=" * 70)


# ========== Phase 1: Scale Pruning ==========
def scale_pruning(model, ratio=0.5):
    """ì±„ë„ë³„ ìŠ¤ì¼€ì¼ì„ 0ìœ¼ë¡œ ë§Œë“¤ì–´ pruning (í•™ìŠµ ì‹œ gradient ìœ ì§€)"""
    print(f"\nğŸ”ª Scale Pruning ({ratio*100:.0f}% ì±„ë„ ì œê±°)...")
    
    pruned_channels = 0
    total_channels = 0
    
    for name, module in model.named_modules():
        if 'mlp' in name and isinstance(module, nn.Linear) and 'proj' in name:
            # requires_gradë¥¼ Falseë¡œ ì„¤ì • (gradient ëŠê¸°)
            with torch.no_grad():
                weight = module.weight.data
                
                # ì¶œë ¥ ì±„ë„ë³„ L2 norm
                channel_norms = torch.norm(weight, p=2, dim=1)
                
                # í•˜ìœ„ ratio% ì±„ë„ ì°¾ê¸°
                num_to_prune = int(weight.size(0) * ratio)
                threshold = torch.kthvalue(channel_norms, num_to_prune)[0]
                
                # Mask ìƒì„±
                keep_mask = channel_norms > threshold
                
                # ì•½í•œ ì±„ë„ë“¤ì„ 0ìœ¼ë¡œ (gradient ëŠê¸´ ìƒíƒœì—ì„œ ìˆ˜ì •)
                weight[~keep_mask] = 0
                
                if module.bias is not None:
                    module.bias.data[~keep_mask] = 0
                
                pruned_channels += (~keep_mask).sum().item()
                total_channels += weight.size(0)
    
    print(f"âœ“ Scale Pruning ì™„ë£Œ!")
    print(f"   - Pruned: {pruned_channels:,}/{total_channels:,} ({pruned_channels/total_channels*100:.1f}%)")
    
    return model


# ========== Phase 2: Distillation ==========
class DistillationTrainer(Trainer):
    def __init__(self, teacher_model, temperature, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.teacher = teacher_model
        self.temperature = temperature
    
    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):
        student_outputs = model(**inputs)
        student_logits = student_outputs.logits
        
        with torch.no_grad():
            teacher_outputs = self.teacher(**inputs)
            teacher_logits = teacher_outputs.logits
        
        loss_fct = torch.nn.KLDivLoss(reduction="batchmean")
        loss = loss_fct(
            F.log_softmax(student_logits / self.temperature, dim=-1),
            F.softmax(teacher_logits / self.temperature, dim=-1)
        ) * (self.temperature ** 2)
        
        return (loss, student_outputs) if return_outputs else loss


def distillation_phase(student_model, tokenizer):
    """Phase 2: Distillation (Pruning ì†ì‹¤ ë³µêµ¬)"""
    print("\n" + "=" * 70)
    print("[PHASE 2/3] ğŸ“ DISTILLATION (Pruning ì†ì‹¤ ë³µêµ¬)")
    print("=" * 70)
    
    # Teacher (ì›ë³¸)
    print("Teacher ë¡œë”©...")
    teacher_model = AutoModelForCausalLM.from_pretrained(
        MODEL_ID,
        torch_dtype=torch.bfloat16,
        device_map="auto"
    )
    teacher_model.eval()
    
    # Data
    print("ë°ì´í„° ì¤€ë¹„...")
    ds = load_dataset(DATASET_ID, split=f"train[:{NUM_DISTILL_SAMPLES}]")
    ds = ds.map(lambda x: {
        "text": tokenizer.apply_chat_template(x["conversations"], add_generation_prompt=True, tokenize=False)
    })
    
    tokenized_ds = ds.map(
        lambda x: tokenizer(x["text"], truncation=True, max_length=MAX_SEQUENCE_LENGTH, padding="max_length"),
        batched=True,
        remove_columns=ds.column_names
    )
    
    # Training
    print("Distillation ì‹œì‘...")
    training_args = TrainingArguments(
        output_dir="./distill_temp",
        num_train_epochs=NUM_EPOCHS,
        per_device_train_batch_size=2,
        learning_rate=LEARNING_RATE,
        warmup_steps=100,
        logging_steps=100,
        save_steps=1000,
        bf16=True,
        report_to="none",
    )
    
    trainer = DistillationTrainer(
        teacher_model=teacher_model,
        temperature=TEMPERATURE,
        model=student_model,
        args=training_args,
        train_dataset=tokenized_ds,
        tokenizer=tokenizer,
    )
    
    trainer.train()
    print("âœ“ Distillation ì™„ë£Œ (Pruning ì†ì‹¤ ë³µêµ¬ë¨)")
    
    # Cleanup
    del teacher_model
    torch.cuda.empty_cache()
    
    return student_model


# ========== Phase 3: Quantization ==========
def quantization_phase(model, tokenizer):
    """Phase 3: Quantization"""
    print("\n" + "=" * 70)
    print("[PHASE 3/3] QUANTIZATION")
    print("=" * 70)
    
    # Data (ë‹¤ë¥¸ ìƒ˜í”Œ)
    ds = load_dataset(DATASET_ID, split=f"train[{NUM_DISTILL_SAMPLES}:{NUM_DISTILL_SAMPLES + NUM_CALIBRATION_SAMPLES}]")
    ds = ds.map(lambda x: {
        "text": tokenizer.apply_chat_template(x["conversations"], add_generation_prompt=True, tokenize=False)
    })
    
    # Uniform 4-bit GPTQ
    recipe = [
        GPTQModifier(
            targets=["Linear"],
            ignore=["lm_head"],
            config_groups={
                "group_0": {
                    "targets": ["Linear"],
                    "weights": {
                        "num_bits": 4,
                        "type": "int",
                        "symmetric": True,
                        "strategy": "group",
                        "group_size": 128
                    }
                }
            }
        )
    ]
    
    print("GPTQ 4-bit ì§„í–‰ ì¤‘...")
    oneshot(
        model=model,
        dataset=ds,
        recipe=recipe,
        max_seq_length=MAX_SEQUENCE_LENGTH,
        num_calibration_samples=NUM_CALIBRATION_SAMPLES,
    )
    
    print("âœ“ Quantization ì™„ë£Œ")
    return model


# ========== Save ==========
def save_model(model, tokenizer):
    """ì €ì¥"""
    print("\n" + "=" * 70)
    print("[FINAL] ì €ì¥")
    print("=" * 70)
    
    os.makedirs(OUT_DIR, exist_ok=True)
    model.save_pretrained(OUT_DIR, save_compressed=True)
    tokenizer.save_pretrained(OUT_DIR)
    
    # Cleanup
    if os.path.exists("./distill_temp"):
        shutil.rmtree("./distill_temp")
    
    if os.path.exists("./model"):
        shutil.rmtree("./model")
    shutil.copytree(OUT_DIR, "./model")
    shutil.make_archive("submit", "zip", ".", "model")
    
    zip_size = os.path.getsize("submit.zip") / (1024**2)



# ========== Main ==========
def main():
    print("\n[PHASE 1/3] ğŸ”ª SCALE PRUNING")
    print("=" * 70)
    
    # Load
    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)
    model = AutoModelForCausalLM.from_pretrained(MODEL_ID, torch_dtype=torch.bfloat16)
    
    # Phase 1: Scale Pruning
    model = scale_pruning(model, ratio=PRUNING_RATIO)
    
    # Phase 2: Distillation
    model = distillation_phase(model, tokenizer)
    
    # Phase 3: Quantization
    model = quantization_phase(model, tokenizer)
    
    # Save
    save_model(model, tokenizer)


if __name__ == "__main__":
    main()
