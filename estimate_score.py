"""
Score Estimator
ì‹¤ì œ ëª¨ë¸ì„ í‰ê°€í•˜ì—¬ ì˜ˆìƒ ì ìˆ˜ ê³„ì‚°

ì‚¬ìš©ë²•:
    python estimate_score.py <model_dir>

ì˜ˆì‹œ:
    python estimate_score.py ./model_exp25
"""

import os
import sys
import time
import json
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from datasets import load_dataset


# ==================== Configuration ====================
BASELINE_MODEL_PATH = "../Model/EXAONE-4.0-1.2B"
DATASET_ID = "LGAI-EXAONE/MANTA-1M"
NUM_EVAL_SAMPLES = 100  # í‰ê°€ ìƒ˜í”Œ ìˆ˜ (ë¹ ë¥¸ ì¶”ì •)
MAX_NEW_TOKENS = 256    # í‰ê°€ìš©


# ==================== Baseline Metrics (ë¯¸ë¦¬ ê³„ì‚°) ====================
# ì‹¤ì œ baseline ëª¨ë¸ë¡œ ì¸¡ì •í•œ ê°’ë“¤
BASELINE_PERF = 1.0  # ê¸°ì¤€ (normalized)
BASELINE_TIME_PER_TOKEN = 0.015  # ì´ˆë‹¹ í† í° (ì˜ˆì‹œ)
BASELINE_TOKENS_PER_SAMPLE = 150  # í‰ê·  ìƒì„± í† í°


def load_model_and_tokenizer(model_path):
    """ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ"""
    print(f"\nëª¨ë¸ ë¡œë”©: {model_path}")
    
    tokenizer = AutoTokenizer.from_pretrained(
        model_path,
        trust_remote_code=True
    )
    
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        torch_dtype=torch.bfloat16,
        device_map="auto",
        trust_remote_code=True
    )
    
    print(f"âœ“ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    
    # ëª¨ë¸ í¬ê¸° ê³„ì‚°
    param_size = sum(p.numel() * p.element_size() for p in model.parameters()) / (1024**2)
    print(f"   - ëª¨ë¸ í¬ê¸°: {param_size:.1f} MB")
    
    return model, tokenizer


def load_eval_dataset(tokenizer, num_samples=100):
    """í‰ê°€ ë°ì´í„°ì…‹ ë¡œë“œ"""
    print(f"\ní‰ê°€ ë°ì´í„° ë¡œë”© ({num_samples} samples)...")
    
    # MANTA ë°ì´í„°ì…‹ì˜ ë’·ë¶€ë¶„ ì‚¬ìš© (í•™ìŠµì— ì‚¬ìš© ì•ˆ í•œ ë¶€ë¶„)
    ds = load_dataset(
        DATASET_ID,
        split=f"train[-{num_samples}:]"  # ë§ˆì§€ë§‰ 100ê°œ
    )
    
    eval_data = []
    for example in ds:
        # Chat template ì ìš©
        prompt = tokenizer.apply_chat_template(
            example["conversations"][:-1],  # ë§ˆì§€ë§‰ ë‹µë³€ ì œì™¸
            add_generation_prompt=True,
            tokenize=False
        )
        
        # ì •ë‹µ (reference)
        reference = example["conversations"][-1]["content"]
        
        eval_data.append({
            "prompt": prompt,
            "reference": reference
        })
    
    print(f"ë°ì´í„° ë¡œë“œ ì™„ë£Œ")
    return eval_data


def evaluate_performance(model, tokenizer, eval_data):
    """ì„±ëŠ¥(PerfNorm) í‰ê°€"""
    print(f"\nì„±ëŠ¥ í‰ê°€ ì¤‘...")
    
    model.eval()
    
    total_score = 0.0
    total_samples = len(eval_data)
    
    for i, item in enumerate(eval_data):
        if i % 10 == 0:
            print(f"   ì§„í–‰: {i}/{total_samples}")
        
        # ìƒì„±
        inputs = tokenizer(item["prompt"], return_tensors="pt").to(model.device)
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=MAX_NEW_TOKENS,
                do_sample=False,
                pad_token_id=tokenizer.pad_token_id
            )
        
        # ìƒì„±ëœ í…ìŠ¤íŠ¸
        generated = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)
        reference = item["reference"]
        
        # ê°„ë‹¨í•œ ì ìˆ˜ (ì‹¤ì œëŠ” ë” ë³µì¡í•œ ë©”íŠ¸ë¦­ ì‚¬ìš©)
        # ROUGE-L, BLEU ë“±ì„ ì‚¬ìš©í•˜ì§€ë§Œ ì—¬ê¸°ì„œëŠ” ë‹¨ìˆœí™”
        score = calculate_simple_score(generated, reference)
        total_score += score
    
    avg_score = total_score / total_samples
    
    print(f"âœ“ ì„±ëŠ¥ í‰ê°€ ì™„ë£Œ")
    print(f"   - í‰ê·  ì ìˆ˜: {avg_score:.4f}")
    
    return avg_score


def calculate_simple_score(generated, reference):
    """ê°„ë‹¨í•œ ìœ ì‚¬ë„ ì ìˆ˜ (0-1)"""
    # ì‹¤ì œë¡œëŠ” ROUGE, BLEU ë“± ì‚¬ìš©
    # ì—¬ê¸°ì„œëŠ” ë‹¨ì–´ overlapìœ¼ë¡œ ê·¼ì‚¬
    
    gen_words = set(generated.lower().split())
    ref_words = set(reference.lower().split())
    
    if len(ref_words) == 0:
        return 0.0
    
    overlap = len(gen_words & ref_words)
    recall = overlap / len(ref_words)
    precision = overlap / len(gen_words) if len(gen_words) > 0 else 0
    
    # F1 score
    if recall + precision == 0:
        return 0.0
    
    f1 = 2 * (precision * recall) / (precision + recall)
    return f1


def evaluate_speed(model, tokenizer, eval_data):
    """ì†ë„(SpeedNorm) í‰ê°€"""
    print(f"\nì†ë„ í‰ê°€ ì¤‘...")
    
    model.eval()
    
    total_time = 0.0
    total_tokens = 0
    num_samples = min(20, len(eval_data))  # ì†ë„ ì¸¡ì •ì€ 20ê°œë§Œ
    
    for i, item in enumerate(eval_data[:num_samples]):
        if i % 5 == 0:
            print(f"   ì§„í–‰: {i}/{num_samples}")
        
        inputs = tokenizer(item["prompt"], return_tensors="pt").to(model.device)
        
        # ì‹œê°„ ì¸¡ì •
        torch.cuda.synchronize() if torch.cuda.is_available() else None
        start_time = time.time()
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=MAX_NEW_TOKENS,
                do_sample=False,
                pad_token_id=tokenizer.pad_token_id
            )
        
        torch.cuda.synchronize() if torch.cuda.is_available() else None
        end_time = time.time()
        
        # ìƒì„±ëœ í† í° ìˆ˜
        generated_tokens = outputs.shape[1] - inputs.input_ids.shape[1]
        
        total_time += (end_time - start_time)
        total_tokens += generated_tokens
    
    avg_time_per_token = total_time / total_tokens if total_tokens > 0 else 0
    avg_tokens_per_second = total_tokens / total_time if total_time > 0 else 0
    
    print(f"âœ“ ì†ë„ í‰ê°€ ì™„ë£Œ")
    print(f"   - í‰ê·  ìƒì„± ì‹œê°„: {total_time/num_samples:.3f} sec/sample")
    print(f"   - í‰ê·  í† í°/ì´ˆ: {avg_tokens_per_second:.1f}")
    print(f"   - ì‹œê°„/í† í°: {avg_time_per_token:.4f} sec")
    
    return avg_time_per_token, avg_tokens_per_second


def calculate_normalized_scores(perf_score, time_per_token):
    """Normalized ì ìˆ˜ ê³„ì‚°"""
    
    # PerfNorm = model_perf / baseline_perf
    perf_norm = perf_score / BASELINE_PERF
    
    # SpeedNorm = 1 - (model_time / baseline_time)
    # ë” ë¹ ë¥´ë©´ ë†’ì€ ì ìˆ˜
    speed_ratio = time_per_token / BASELINE_TIME_PER_TOKEN
    speed_norm = max(0, 1 - speed_ratio)
    
    # ìµœì¢… ì ìˆ˜
    final_score = 0.5 * perf_norm + 0.5 * speed_norm
    
    return perf_norm, speed_norm, final_score


def estimate_with_baseline_comparison(model_path):
    """Baselineê³¼ ë¹„êµí•˜ì—¬ ì˜ˆìƒ ì ìˆ˜ ê³„ì‚°"""
    print("=" * 70)
    print("Score Estimator")
    print("=" * 70)
    
    # ëª¨ë¸ ë¡œë“œ
    model, tokenizer = load_model_and_tokenizer(model_path)
    
    # í‰ê°€ ë°ì´í„°
    eval_data = load_eval_dataset(tokenizer, NUM_EVAL_SAMPLES)
    
    # ì„±ëŠ¥ í‰ê°€
    perf_score = evaluate_performance(model, tokenizer, eval_data)
    
    # ì†ë„ í‰ê°€
    time_per_token, tokens_per_sec = evaluate_speed(model, tokenizer, eval_data)
    
    # Normalized ì ìˆ˜ ê³„ì‚°
    perf_norm, speed_norm, final_score = calculate_normalized_scores(
        perf_score, time_per_token
    )
    
    # ê²°ê³¼ ì¶œë ¥
    print("\n" + "=" * 70)
    print("ğŸ“Š ìµœì¢… ê²°ê³¼")
    print("=" * 70)
    print(f"\nì„±ëŠ¥ ë©”íŠ¸ë¦­:")
    print(f"   - Raw Score: {perf_score:.4f}")
    print(f"   - PerfNorm: {perf_norm:.4f}")
    
    print(f"\nì†ë„ ë©”íŠ¸ë¦­:")
    print(f"   - ì‹œê°„/í† í°: {time_per_token:.4f} sec")
    print(f"   - í† í°/ì´ˆ: {tokens_per_sec:.1f}")
    print(f"   - SpeedNorm: {speed_norm:.4f}")
    
    print(f"\nìµœì¢… ì ìˆ˜:")
    print(f"   Score = 0.5 Ã— {perf_norm:.4f} + 0.5 Ã— {speed_norm:.4f}")
    print(f"   Score = {final_score:.4f}")
    
    # íŒŒì¼ë¡œ ì €ì¥
    result = {
        "model_path": model_path,
        "perf_score": float(perf_score),
        "perf_norm": float(perf_norm),
        "time_per_token": float(time_per_token),
        "tokens_per_sec": float(tokens_per_sec),
        "speed_norm": float(speed_norm),
        "final_score": float(final_score)
    }
    
    result_file = os.path.join(model_path, "estimated_score.json")
    with open(result_file, 'w') as f:
        json.dump(result, f, indent=2)
    
    print(f"\nâœ“ ê²°ê³¼ ì €ì¥: {result_file}")
    print("=" * 70)
    
    return result


def quick_estimate(model_path):
    """ë¹ ë¥¸ ì¶”ì • (ê°„ë‹¨í•œ ë©”íŠ¸ë¦­ë§Œ)"""
    print("=" * 70)
    print("Quick Score Estimator (ë¹ ë¥¸ ì¶”ì •)")
    print("=" * 70)
    
    print(f"\nëª¨ë¸ ë¶„ì„: {model_path}")
    
    # ëª¨ë¸ íŒŒì¼ í¬ê¸°
    model_files = []
    for root, dirs, files in os.walk(model_path):
        for file in files:
            if file.endswith(('.bin', '.safetensors')):
                filepath = os.path.join(root, file)
                size_mb = os.path.getsize(filepath) / (1024**2)
                model_files.append((file, size_mb))
    
    total_size = sum(size for _, size in model_files)
    
    print(f"   - ëª¨ë¸ í¬ê¸°: {total_size:.1f} MB")
    
    # ê°„ë‹¨í•œ ì¶”ì •
    # ì›ë³¸: 2400 MB
    # ì••ì¶•ë¥  ê¸°ë°˜ SpeedNorm ì¶”ì •
    compression_ratio = 2400 / total_size if total_size > 0 else 1
    
    # ê²½í—˜ì  ê³µì‹
    estimated_speed_norm = min(0.9, 0.2 + 0.15 * compression_ratio)
    estimated_perf_norm = 0.95 - 0.05 * (compression_ratio - 1)  # ì••ì¶•í• ìˆ˜ë¡ ì•½ê°„ ì„±ëŠ¥ í•˜ë½
    estimated_score = 0.5 * estimated_perf_norm + 0.5 * estimated_speed_norm
    
    print(f"\nì¶”ì • ê²°ê³¼ (ê°„ë‹¨):")
    print(f"   - ì••ì¶•ë¥ : {compression_ratio:.2f}x")
    print(f"   - ì˜ˆìƒ PerfNorm: ~{estimated_perf_norm:.3f}")
    print(f"   - ì˜ˆìƒ SpeedNorm: ~{estimated_speed_norm:.3f}")
    print(f"   - ì˜ˆìƒ Score: ~{estimated_score:.3f}")
    
    print(f"\nğŸ’¡ ë” ì •í™•í•œ ì¶”ì •ì„ ìœ„í•´ì„œëŠ”:")
    print(f"   python estimate_score.py {model_path} --full")
    print("=" * 70)


# ==================== Main ====================
if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("ì‚¬ìš©ë²•:")
        print("  python estimate_score.py <model_dir> [--quick]")
        print("\nì˜ˆì‹œ:")
        print("  python estimate_score.py ./model_exp25")
        print("  python estimate_score.py ./model_exp25 --quick  # ë¹ ë¥¸ ì¶”ì •")
        sys.exit(1)
    
    model_path = sys.argv[1]
    
    if not os.path.exists(model_path):
        print(f"ëª¨ë¸ ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {model_path}")
        sys.exit(1)
    
    # Quick mode
    if "--quick" in sys.argv:
        quick_estimate(model_path)
    else:
        # Full evaluation
        estimate_with_baseline_comparison(model_path)
