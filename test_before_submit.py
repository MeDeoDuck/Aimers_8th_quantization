"""
ì œì¶œ ì „ ì¢…í•© í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
ì‹¤í–‰: python test_before_submit.py --model ./model --zip exp6_submit.zip
"""

import os
import time
import torch
import zipfile
import argparse
from pathlib import Path
from transformers import AutoModelForCausalLM, AutoTokenizer

def get_model_size(model_path):
    """ëª¨ë¸ í¬ê¸° ê³„ì‚°"""
    total_size = 0
    for file in Path(model_path).rglob('*'):
        if file.is_file():
            total_size += file.stat().st_size
    return total_size / (1024**3)

def test_model_loading(model_path):
    """ëª¨ë¸ ë¡œë”© í…ŒìŠ¤íŠ¸"""
    print("\n" + "="*50)
    print("1. ëª¨ë¸ ë¡œë”© í…ŒìŠ¤íŠ¸")
    print("="*50)
    
    try:
        tokenizer = AutoTokenizer.from_pretrained(
            model_path, 
            trust_remote_code=True
        )
        print("âœ… Tokenizer ë¡œë“œ ì„±ê³µ")
        
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            trust_remote_code=True,
            torch_dtype=torch.bfloat16,
            device_map="auto"
        )
        print("âœ… Model ë¡œë“œ ì„±ê³µ")
        
        return tokenizer, model, True
    except Exception as e:
        print(f"âŒ ë¡œë”© ì‹¤íŒ¨: {e}")
        return None, None, False

def test_inference(tokenizer, model):
    """ì¶”ë¡  í…ŒìŠ¤íŠ¸"""
    print("\n" + "="*50)
    print("2. ì¶”ë¡  í…ŒìŠ¤íŠ¸")
    print("="*50)
    
    test_prompts = [
        "Hello, how are you?",
        "Explain machine learning.",
        "Write a poem about AI.",
    ]
    
    try:
        for i, prompt in enumerate(test_prompts):
            inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
            
            start = time.time()
            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=30,
                    do_sample=False,
                )
            end = time.time()
            
            generated = tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            print(f"\ní…ŒìŠ¤íŠ¸ {i+1}:")
            print(f"ì…ë ¥: {prompt}")
            print(f"ì¶œë ¥: {generated[:100]}...")
            print(f"ì‹œê°„: {end-start:.2f}ì´ˆ")
        
        print("\nâœ… ì¶”ë¡  í…ŒìŠ¤íŠ¸ í†µê³¼")
        return True
    except Exception as e:
        print(f"âŒ ì¶”ë¡  ì‹¤íŒ¨: {e}")
        return False

def test_model_size(model_path):
    """ëª¨ë¸ í¬ê¸° í™•ì¸"""
    print("\n" + "="*50)
    print("3. ëª¨ë¸ í¬ê¸° í™•ì¸")
    print("="*50)
    
    size_gb = get_model_size(model_path)
    print(f"ëª¨ë¸ í¬ê¸°: {size_gb:.2f} GB")
    
    # ê¸°ì¤€ í¬ê¸° (EXAONE-4.0-1.2B ì›ë³¸ì€ ì•½ 2.4GB)
    if size_gb > 2.5:
        print("âš ï¸  ì›ë³¸ë³´ë‹¤ í¼ (ì–‘ìí™” íš¨ê³¼ ì—†ìŒ?)")
    elif size_gb < 0.5:
        print("âš ï¸  ë„ˆë¬´ ì‘ìŒ (ë¬¸ì œ ìˆì„ ìˆ˜ ìˆìŒ)")
    else:
        print("âœ… ì ì • í¬ê¸°")
    
    return size_gb

def validate_zip(zip_path):
    """ì œì¶œ íŒŒì¼ ê²€ì¦"""
    print("\n" + "="*50)
    print("4. ì œì¶œ íŒŒì¼ ê²€ì¦")
    print("="*50)
    
    if not os.path.exists(zip_path):
        print(f"âŒ {zip_path} íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤!")
        return False
    
    # í¬ê¸° í™•ì¸
    size_gb = os.path.getsize(zip_path) / (1024**3)
    print(f"ì••ì¶• íŒŒì¼ í¬ê¸°: {size_gb:.2f} GB")
    
    if size_gb > 10:
        print("âŒ 10GB ì´ˆê³¼! ì œì¶œ ë¶ˆê°€!")
        return False
    else:
        print("âœ… í¬ê¸° ì œí•œ í†µê³¼")
    
    # ë‚´ë¶€ êµ¬ì¡° í™•ì¸
    with zipfile.ZipFile(zip_path, 'r') as zf:
        file_list = zf.namelist()
        
        # í•„ìˆ˜ íŒŒì¼
        required = ['model/config.json']
        missing = [f for f in required if f not in file_list]
        
        if missing:
            print(f"âŒ í•„ìˆ˜ íŒŒì¼ ëˆ„ë½: {missing}")
            return False
        
        # safetensors í™•ì¸
        safetensors = [f for f in file_list if 'safetensors' in f]
        if not safetensors:
            print("âŒ safetensors íŒŒì¼ ì—†ìŒ!")
            return False
        
        print(f"âœ… safetensors íŒŒì¼: {len(safetensors)}ê°œ")
        
        # êµ¬ì¡° í™•ì¸
        if all(f.startswith('model/') for f in file_list):
            print("âœ… ë””ë ‰í† ë¦¬ êµ¬ì¡° ì •ìƒ")
        else:
            print("âŒ model/ ì™¸ë¶€ì— íŒŒì¼ ìˆìŒ!")
            return False
    
    print("âœ… ì œì¶œ íŒŒì¼ ê²€ì¦ í†µê³¼!")
    return True

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--model', type=str, required=True, help='ëª¨ë¸ ê²½ë¡œ')
    parser.add_argument('--zip', type=str, help='ì œì¶œí•  zip íŒŒì¼ ê²½ë¡œ')
    args = parser.parse_args()
    
    print("="*50)
    print("ì œì¶œ ì „ ì¢…í•© í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("="*50)
    
    results = {
        'loading': False,
        'inference': False,
        'size': False,
        'zip': False,
    }
    
    # 1. ë¡œë”© í…ŒìŠ¤íŠ¸
    tokenizer, model, results['loading'] = test_model_loading(args.model)
    
    # 2. ì¶”ë¡  í…ŒìŠ¤íŠ¸
    if results['loading']:
        results['inference'] = test_inference(tokenizer, model)
    
    # 3. í¬ê¸° í…ŒìŠ¤íŠ¸
    size_gb = test_model_size(args.model)
    results['size'] = (0.5 < size_gb < 2.5)
    
    # 4. ZIP ê²€ì¦
    if args.zip:
        results['zip'] = validate_zip(args.zip)
    
    # ìµœì¢… ê²°ê³¼
    print("\n" + "="*50)
    print("ìµœì¢… ê²°ê³¼")
    print("="*50)
    
    for test, passed in results.items():
        status = "âœ… í†µê³¼" if passed else "âŒ ì‹¤íŒ¨"
        print(f"{test}: {status}")
    
    if all(results.values()):
        print("\nğŸ‰ ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼! ì œì¶œ ê°€ëŠ¥í•©ë‹ˆë‹¤!")
    else:
        print("\nâš ï¸  ì¼ë¶€ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨. í™•ì¸ í›„ ìˆ˜ì •í•˜ì„¸ìš”.")

if __name__ == "__main__":
    main()
